{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d71e86c4aa66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautocorrelation_plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimg_prep\u001b[0m \u001b[1;31m#image preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'app.model'"
     ]
    }
   ],
   "source": [
    "#Solution adapted from Kris C Naik\n",
    "#GitHub https://github.com/krishnaik06/Stock-Price-Prediction-using-Keras-and-Recurrent-Neural-Networ/blob/master/rnn.py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "#from app.model.preprocessor import Preprocessor as img_prep #image preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_datareader as dr\n",
    "from datetime import datetime\n",
    "import pandas_datareader.data as web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start=datetime(2015,1,1)\n",
    "end=datetime(2019,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>47.419998</td>\n",
       "      <td>46.540001</td>\n",
       "      <td>46.660000</td>\n",
       "      <td>46.759998</td>\n",
       "      <td>27913900.0</td>\n",
       "      <td>42.418739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>46.730000</td>\n",
       "      <td>46.250000</td>\n",
       "      <td>46.369999</td>\n",
       "      <td>46.330002</td>\n",
       "      <td>39673900.0</td>\n",
       "      <td>42.028660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>46.750000</td>\n",
       "      <td>45.540001</td>\n",
       "      <td>46.380001</td>\n",
       "      <td>45.650002</td>\n",
       "      <td>36447900.0</td>\n",
       "      <td>41.411785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>46.459999</td>\n",
       "      <td>45.490002</td>\n",
       "      <td>45.980000</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>29114100.0</td>\n",
       "      <td>41.937946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>47.750000</td>\n",
       "      <td>46.720001</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>47.590000</td>\n",
       "      <td>29645200.0</td>\n",
       "      <td>43.171677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low       Open      Close      Volume  Adj Close\n",
       "Date                                                                         \n",
       "2015-01-02  47.419998  46.540001  46.660000  46.759998  27913900.0  42.418739\n",
       "2015-01-05  46.730000  46.250000  46.369999  46.330002  39673900.0  42.028660\n",
       "2015-01-06  46.750000  45.540001  46.380001  45.650002  36447900.0  41.411785\n",
       "2015-01-07  46.459999  45.490002  45.980000  46.230000  29114100.0  41.937946\n",
       "2015-01-08  47.750000  46.720001  46.750000  47.590000  29645200.0  43.171677"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tesla = web.DataReader('TSLA', 'google', start,end)\n",
    "microsoft = dr.data.get_data_yahoo('MSFT', start=start, end=end)\n",
    "\n",
    "dataset = microsoft\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for loading our saved model and classifying new images\n",
    "class LiteOCR:\n",
    "    def __init__(self, fn=\"alpha_weights.pkl\", pool_size=2):\n",
    "        #load the weights from the pickle file and the meta data\n",
    "        [weights, meta] = pickle.load(open(fn, 'rb'), encoding='latin1') #currently, this class MUST be initialized from a pickle file\n",
    "        #list to store labels\n",
    "        self.vocab = meta[\"vocab\"]\n",
    "        \n",
    "        #how many rows and columns in an image\n",
    "        self.img_rows = meta[\"img_side\"] ; self.img_cols = meta[\"img_side\"]\n",
    "        \n",
    "        #load our CNN\n",
    "        self.CNN = LiteCNN()\n",
    "        #with our saved weights\n",
    "        self.CNN.load_weights(weights)\n",
    "        #define the pooling layers size\n",
    "        self.CNN.pool_size=int(pool_size)\n",
    "    \n",
    "    #classify new image\n",
    "    def predict(self, image):\n",
    "        print(image.shape)\n",
    "        #vectorize the image into the right shape for our network\n",
    "        X = np.reshape(image, (1, 1, self.img_rows, self.img_cols))\n",
    "        X = X.astype(\"float32\")\n",
    "        \n",
    "        #make the prediction\n",
    "        predicted_i = self.CNN.predict(X)\n",
    "        #return the predicted label\n",
    "        return self.vocab[predicted_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteCNN:\n",
    "    def __init__(self):\n",
    "        sel.layers=[]\n",
    "        #size of pooling area for max pooling\n",
    "        self.pool_size=None\n",
    "\n",
    "    def load_weight(self, weights):\n",
    "        assert not self.layers, \"Weights can only be loaded once!\"\n",
    "        #add the saved matrix values to the convolutional network\n",
    "        for k in range(len(weights.keys())):\n",
    "            self.layers.append(weights['layer{}'.format(k)])\n",
    "    def predict(self, X):\n",
    "        #here is where the network magic happens at a high level\n",
    "        h=self.cnn_layer(Xm, layer_i=0,border_mode='full'); X=h\n",
    "        h = self.relu_layer(X); X=h;\n",
    "        h = self.cnn_layer(X, layer_i=2, border_mode='valid'); X=h\n",
    "        h = self.relu_layer(X); X=h;\n",
    "        h = self.maxpooling_layer(X); x=h;\n",
    "        h = self.dropout_layer(X, .25); x=h;\n",
    "        h = self.flatten_layer(X, layer_i=7); X = h;\n",
    "        h = self.dense_layer(X, fully, layer_i=0); X = h;\n",
    "        h = self.softmax_layer2D(X); x = h;\n",
    "        max_i = self.classify(X)\n",
    "        return max_i[0]\n",
    "    \n",
    "#given our feature map we've learned from convolving around the image\n",
    "    #lets make it more dense by performing pooling, specifically max pooling\n",
    "    #we'll select the max values from the image matrix and use that as our new feature map\n",
    "    def maxpooling_layer(self, convolved_features):\n",
    "        #given our learned features and images\n",
    "        nb_features = convolved_features.shape[0]\n",
    "        nb_images = convolved_features.shape[1]\n",
    "        conv_dim = convolved_features.shape[2]\n",
    "        res_dim = int(conv_dim / self.pool_size)       #assumed square shape\n",
    "\n",
    "        #initialize our more dense feature list as empty\n",
    "        pooled_features = np.zeros((nb_features, nb_images, res_dim, res_dim))\n",
    "        #for each image\n",
    "        for image_i in range(nb_images):\n",
    "            #and each feature map\n",
    "            for feature_i in range(nb_features):\n",
    "                #begin by the row\n",
    "                for pool_row in range(res_dim):\n",
    "                    #define start and end points\n",
    "                    row_start = pool_row * self.pool_size\n",
    "                    row_end   = row_start + self.pool_size\n",
    "\n",
    "                    #for each column (so its a 2D iteration)\n",
    "                    for pool_col in range(res_dim):\n",
    "                        #define start and end points\n",
    "                        col_start = pool_col * self.pool_size\n",
    "                        col_end   = col_start + self.pool_size\n",
    "\n",
    "                        #define a patch given our defined starting ending points\n",
    "                        patch = convolved_features[feature_i, image_i, row_start : row_end,col_start : col_end]\n",
    "                        #then take the max value from that patch\n",
    "                        #store it. this is our new learned feature/filter\n",
    "                        pooled_features[feature_i, image_i, pool_row, pool_col] = np.max(patch)\n",
    "        return pooled_features\n",
    "\n",
    "    #convolution is the most important of the matrix operations here\n",
    "    #well define our input, lauyer number, and a border mode (explained below)\n",
    "    def cnn_layer(self, X, layer_i=0, border_mode = \"full\"):\n",
    "        #we'll store our feature maps and bias value in these 2 vars\n",
    "        features = self.layers[layer_i][\"param_0\"]\n",
    "        bias = self.layers[layer_i][\"param_1\"]\n",
    "        #how big is our filter/patch?\n",
    "        patch_dim = features[0].shape[-1]\n",
    "        #how many features do we have?\n",
    "        nb_features = features.shape[0]\n",
    "        #How big is our image?\n",
    "        image_dim = X.shape[2] #assume image square\n",
    "        #R G B values\n",
    "        image_channels = X.shape[1]\n",
    "        #how many images do we have?\n",
    "        nb_images = X.shape[0]\n",
    "\n",
    "        #With border mode \"full\" you get an output that is the \"full\" size as the input. \n",
    "        #That means that the filter has to go outside the bounds of the input by \"filter size / 2\" - \n",
    "        #the area outside of the input is normally padded with zeros.\n",
    "        if border_mode == \"full\":\n",
    "            conv_dim = image_dim + patch_dim - 1\n",
    "        #With border mode \"valid\" you get an output that is smaller than the input because \n",
    "        #the convolution is only computed where the input and the filter fully overlap.\n",
    "        elif border_mode == \"valid\":\n",
    "            conv_dim = image_dim - patch_dim + 1\n",
    "\n",
    "        #we'll initialize our feature matrix\n",
    "        convolved_features = np.zeros((nb_images, nb_features, conv_dim, conv_dim));\n",
    "        #then we'll iterate through each image that we have\n",
    "        for image_i in range(nb_images):\n",
    "            #for each feature \n",
    "            for feature_i in range(nb_features):\n",
    "                #lets initialize a convolved image as empty\n",
    "                convolved_image = np.zeros((conv_dim, conv_dim))\n",
    "                #then for each channel (r g b )\n",
    "                for channel in range(image_channels):\n",
    "                    #lets extract a feature from our feature map\n",
    "                    feature = features[feature_i, channel, :, :]\n",
    "                    #then define a channel specific part of our image\n",
    "                    image   = X[image_i, channel, :, :]\n",
    "                    #perform convolution on our image, using a given feature filter\n",
    "                    convolved_image += self.convolve2d(image, feature, border_mode);\n",
    "\n",
    "                #add a bias to our convoved image\n",
    "                convolved_image = convolved_image + bias[feature_i]\n",
    "                #add it to our list of convolved features (learnings)\n",
    "                convolved_features[image_i, feature_i, :, :] = convolved_image\n",
    "        return convolved_features\n",
    "\n",
    "    #In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "    def dense_layer(self, X, layer_i=0):\n",
    "        #so we'll initialize our weight and bias for this layer\n",
    "        W = self.layers[layer_i][\"param_0\"]\n",
    "        b = self.layers[layer_i][\"param_1\"]\n",
    "        #and multiply it by our input (dot product)\n",
    "        output = np.dot(X, W) + b\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    #so what does the convolution operation look like?, given an image and a feature map (filter)\n",
    "    def convolve2d(image, feature, border_mode=\"full\"):\n",
    "        #we'll define the tensor dimensions of the image and the feature\n",
    "        image_dim = np.array(image.shape)\n",
    "        feature_dim = np.array(feature.shape)\n",
    "        #as well as a target dimension\n",
    "        target_dim = image_dim + feature_dim - 1\n",
    "        #then we'll perform a fast fourier transform on both the input and the filter\n",
    "        #performing a convolution can be written as a for loop but for many convolutions\n",
    "        #this approach is too comp. expensive/slow. it can be performed orders of magnitude\n",
    "        #faster using a fast fourier transform. \n",
    "        fft_result = np.fft.fft2(image, target_dim) * np.fft.fft2(feature, target_dim)\n",
    "        #and set the result to our target \n",
    "        target = np.fft.ifft2(fft_result).real\n",
    "\n",
    "        if border_mode == \"valid\":\n",
    "            # To compute a valid shape, either np.all(x_shape >= y_shape) or\n",
    "            # np.all(y_shape >= x_shape).\n",
    "            #decide a target dimension to convolve around\n",
    "            valid_dim = image_dim - feature_dim + 1\n",
    "            if np.any(valid_dim < 1):\n",
    "                valid_dim = feature_dim - image_dim + 1\n",
    "            start_i = (target_dim - valid_dim) // 2\n",
    "            end_i = start_i + valid_dim\n",
    "            target = target[start_i[0]:end_i[0], start_i[1]:end_i[1]]\n",
    "        return target\n",
    "\n",
    "    def relu_layer(x):\n",
    "        #turn all negative values in a matrix into zeros\n",
    "        z = np.zeros_like(x)\n",
    "        return np.where(x>z,x,z)\n",
    "\n",
    "    def softmax_layer2D(w):\n",
    "        #this function will calculate the probabilities of each\n",
    "        #target class over all possible target classes. \n",
    "        maxes = np.amax(w, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        e = np.exp(w - maxes)\n",
    "        dist = e / np.sum(e, axis=1, keepdims=True)\n",
    "        return dist\n",
    "\n",
    "    #affect the probability a node will be turned off by multiplying it\n",
    "    #by a p values (.25 we define)\n",
    "    def dropout_layer(X, p):\n",
    "        retain_prob = 1. - p\n",
    "        X *= retain_prob\n",
    "        return X\n",
    "\n",
    "    #get the largest probabililty value from the list\n",
    "    def classify(X):\n",
    "        return X.argmax(axis=-1)\n",
    "\n",
    "    #tensor transformation, less dimensions\n",
    "    def flatten_layer(X):\n",
    "        flatX = np.zeros((X.shape[0],np.prod(X.shape[1:])))\n",
    "        for i in range(X.shape[0]):\n",
    "            flatX[i,:] = X[i].flatten(order='C')\n",
    "        return flatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46.65999985 46.36999893 46.38000107 45.97999954 46.75       47.61000061\n",
      "  47.41999817 46.97000122 45.95999908 46.22000122 45.31000137 46.29999924\n",
      "  45.93999863 46.38000107 47.36000061 47.         42.95000076 42.74000168\n",
      "  40.93000031 41.54999924 40.59000015 41.63000107 41.93999863 42.22000122\n",
      "  42.68000031 42.24000168 42.74000168 42.65000153 42.65999985 43.38000107\n",
      "  43.97000122 43.63000107 43.18000031 43.50999832 43.70000076 44.15000153\n",
      "  43.95000076 43.99000168 44.13000107 43.66999817 43.56000137 43.00999832\n",
      "  43.06999969 43.         42.18999863 42.34999847 42.31000137 41.33000183\n",
      "  40.70000076 41.47000122 41.36999893 41.43000031 42.25999832 42.56000137\n",
      "  42.88000107 42.77999878 42.91999817 41.22000122 41.11999893 41.09999847\n",
      "  40.77999878 40.59999847 40.65999985 40.34000015 41.61000061 41.45999908\n",
      "  41.25       41.63000107 41.40000153 41.79999924 41.75999832 41.95000076\n",
      "  41.66999817 41.72999954 43.         42.66999817 42.88999939 45.65999985\n",
      "  47.22999954 47.77999878 48.72000122 48.70000076 48.58000183 48.36999893\n",
      "  47.81999969 47.56999969 46.27000046 47.54999924 47.54999924 46.84999847\n",
      "  48.18999863 48.02999878 48.86999893 47.97999954 47.56000137 47.38999939\n",
      "  47.27999878 47.29999924 46.83000183 46.81999969 47.5        47.43000031\n",
      "  47.06000137 46.93000031 47.36999893 46.79000092 46.31000137 46.29999924\n",
      "  45.75999832 45.79000092 46.65999985 46.22000122 45.45000076 45.34999847\n",
      "  45.72999954 46.22000122 46.79000092 46.33000183 46.13000107 45.66999817\n",
      "  46.02999878 45.65000153 45.04000092 44.70999908 44.45999908 44.47999954\n",
      "  43.95999908 44.34000015 44.43999863 44.75       45.00999832 44.97999954\n",
      "  45.45000076 45.68000031 46.00999832 46.54999924 46.65000153 46.77999878\n",
      "  45.43999863 45.27000046 45.90999985 45.93999863 45.58000183 45.40000153\n",
      "  46.25999832 47.29000092 46.97999954 46.75       47.97999954 47.70999908\n",
      "  46.38999939 46.95000076 46.81999969 46.18999863 47.06000137 46.52999878\n",
      "  46.81000137 46.84000015 46.77999878 46.06999969 45.29999924 40.45000076\n",
      "  42.56999969 42.00999832 43.22999954 43.40000153 43.56000137 42.16999817\n",
      "  42.36000061 43.40999985 42.81000137 43.29999924 44.20999908 43.11999893\n",
      "  43.13999939 43.43000031 43.18999863 43.97000122 44.29000092 43.5\n",
      "  43.61999893 43.38000107 43.93000031 43.45000076 44.47999954 43.83000183\n",
      "  43.36999893 43.88000107 44.75       44.27000046 45.75       46.33000183\n",
      "  47.09999847 46.56000137 47.45000076 46.97999954 46.56000137 46.65000153\n",
      "  47.00999832 47.02000046 47.41999817 47.43999863 47.91999817 47.52999878\n",
      "  52.29999924 52.52999878 53.99000168 53.54000092 53.54000092 53.31999969\n",
      "  52.84999847 52.93000031 54.18000031 54.49000168 54.09000015 54.54999924\n",
      "  54.06999969 53.70000076 53.47999954 53.06999969 53.08000183 53.16999817\n",
      "  53.         53.99000168 54.25       54.25       53.91999817 54.09000015\n",
      "  53.79999924 54.54000092 54.40999985 55.31999969 55.49000168 54.11999893\n",
      "  55.79000092 55.47000122 55.36999893 55.38999939 54.70999908 54.33000183\n",
      "  55.65999985 55.54000092 56.36000061 55.77000046 54.88000107 54.99000168\n",
      "  55.70000076 55.86000061 55.34999847 56.29000092 56.47000122 56.04000092\n",
      "  54.31999969 54.93000031 54.31999969 52.70000076 52.36999893 52.50999832\n",
      "  52.75999832 53.79999924 52.         51.31000137 51.47999954 49.97999954\n",
      "  51.         51.40999985 51.93999863 51.79000092 52.00999832 51.86000061\n",
      "  54.72999954 54.88000107 54.16999817 53.25       52.09999847 51.93999863\n",
      "  49.54999924 49.02000046 49.88999939 48.68000031 50.25       50.90000153\n",
      "  51.49000168 52.33000183 51.97000122 52.27999878 52.34000015 50.68999863\n",
      "  51.72999954 52.59999847 51.34999847 50.97000122 52.40999985 52.97000122\n",
      "  52.40000153 51.56000137 50.79999924 51.88999939 52.93000031 53.\n",
      "  52.70999908 52.75       53.45000076 54.20999908 54.91999817 53.25\n",
      "  53.61000061 54.11000061 53.84000015 54.20999908 53.65999985 54.93000031\n",
      "  54.95000076 55.04999924 55.43000031 55.18999863 54.36000061 54.86999893\n",
      "  54.66999817 54.49000168 54.36999893 55.11999893 55.22000122 55.29999924\n",
      "  55.49000168 56.63000107 56.29000092 55.79999924 51.90999985 51.77999878\n",
      "  52.25999832 51.47999954 50.61999893 49.34999847 50.         50.34000015\n",
      "  49.84000015 49.86999893 49.91999817 50.49000168 50.33000183 51.13000107\n",
      "  51.20000076 51.43999863 50.79999924 51.72000122 50.47999954 50.47000122\n",
      "  50.47999954 50.59999847 50.70000076 51.91999817 51.93000031 51.91999817\n",
      "  52.25999832 52.43999863 52.63999939 52.38000107 51.99000168 52.24000168\n",
      "  52.02000046 52.         51.04999924 49.58000183 49.90000153 49.77999878\n",
      "  49.52000046 50.40999985 50.63999939 50.20000076 51.08000183 51.27999878\n",
      "  49.81000137 49.09999847 48.91999817 49.90999985 50.72000122 51.13000107\n",
      "  50.83000183 50.77999878 51.41999817 51.72999954 52.5        52.93999863\n",
      "  53.56000137 53.84000015 53.95000076 53.70000076 53.70999908 56.15000153\n",
      "  55.97999954 56.08000183 56.47000122 56.52000046 56.61000061 56.\n",
      "  56.25999832 56.59999847 56.84999847 56.68000031 56.79999924 57.65000153\n",
      "  58.06000137 58.16999817 58.15999985 58.02999878 58.02999878 58.00999832\n",
      "  57.61000061 57.54000092 57.41999817 57.43000031 57.59999847 57.90000153\n",
      "  57.79999924 57.88000107 58.27999878 58.18000031 57.97999954 57.65000153\n",
      "  57.00999832 57.66999817 57.77999878 57.47000122 57.63000107 56.79000092\n",
      "  56.         56.5        56.38999939 56.15000153 57.63000107 57.27000046\n",
      "  57.34999847 57.50999832 57.91999817 57.86999893 57.08000183 56.93000031\n",
      "  57.88000107 57.81000137 57.56999969 57.40999985 57.27000046 57.29000092\n",
      "  57.74000168 57.84999847 57.90999985 57.88999939 57.11000061 56.70000076\n",
      "  57.11999893 57.36000061 57.52999878 57.47000122 57.5        60.27999878\n",
      "  59.93999863 60.84999847 60.81000137 60.61000061 60.00999832 60.15999985\n",
      "  59.97000122 59.81999969 59.52999878 58.65000153 59.77999878 60.54999924\n",
      "  60.         60.47999954 58.22999954 59.02000046 58.33000183 58.93999863\n",
      "  60.40999985 60.77999878 60.5        60.97999954 61.00999832 60.29999924\n",
      "  60.34000015 60.65000153 60.86000061 60.11000061 59.08000183 59.70000076\n",
      "  60.43000031 60.00999832 61.29999924 61.18000031 61.81999969 62.5\n",
      "  63.         62.70000076 62.95000076 62.56000137 63.68999863 63.43000031\n",
      "  63.84000015 63.45000076 63.20999908 63.40000153 62.86000061 62.95999908\n",
      "  62.79000092 62.47999954 62.18999863 62.29999924 62.75999832 62.72999954\n",
      "  62.61000061 63.06000137 62.61999893 62.68000031 62.66999817 62.24000168\n",
      "  62.66999817 62.70000076 63.20000076 63.95000076 64.12000275 65.38999939\n",
      "  65.69000244 64.86000061 64.36000061 63.25       63.5        63.5\n",
      "  63.74000168 63.56999969 63.52000046 64.25       64.23999786 64.41000366\n",
      "  64.5        64.73999786 64.47000122 64.61000061 64.33000183 64.41999817\n",
      "  64.52999878 64.54000092 64.08000183 64.12999725 64.69000244 63.99000168\n",
      "  63.97000122 64.19000244 64.26000214 65.19000244 65.11000061 65.01000214\n",
      "  64.52999878 64.55000305 64.75       64.91000366 64.91000366 65.19000244\n",
      "  64.12000275 64.94000244 65.36000061 64.62999725 64.95999908 65.12000275\n",
      "  65.41999817 65.65000153 65.80999756 65.38999939 66.30000305 65.59999847\n",
      "  65.84999847 65.61000061 65.59999847 65.41999817 65.29000092 65.04000092\n",
      "  65.33000183 65.65000153 65.45999908 65.66999817 67.48000336 67.90000153\n",
      "  68.08000183 68.15000153 68.91000366 68.68000031 69.70999908 69.37999725\n",
      "  69.02999878 68.90000153 68.97000122 68.86000061 68.98999786 68.36000061\n",
      "  68.61000061 68.13999939 68.23000336 68.88999939 67.40000153 67.5\n",
      "  67.88999939 68.72000122 68.87000275 68.97000122 69.80000305 69.79000092\n",
      "  70.52999878 70.23999786 70.44000244 71.97000122 72.30000305 72.63999939\n",
      "  72.51000214 72.04000092 69.25       70.01999664 70.91000366 69.26999664\n",
      "  69.73000336 70.5        70.81999969 70.20999908 70.54000092 70.08999634\n",
      "  71.40000153 70.11000061 69.20999908 69.37999725 68.77999878 69.33000183\n",
      "  68.26000214 68.26999664 68.69999695 69.45999908 70.         70.69000244\n",
      "  71.5        72.23999786 72.80000305 73.08999634 73.5        74.18000031\n",
      "  73.44999695 73.52999878 73.80000305 74.33999634 73.76000214 72.66999817\n",
      "  73.30000305 73.09999847 72.55000305 72.19000244 72.40000153 72.80000305\n",
      "  72.08999634 72.25       71.90000153 71.61000061 73.05999756 73.58999634\n",
      "  73.33999634 73.58000183 72.26999664 72.47000122 72.34999847 72.95999908\n",
      "  72.73999786 72.86000061 73.05999756 72.25       73.01000214 74.02999878\n",
      "  74.70999908 73.33999634 73.73999786 73.68000031 74.33000183 74.30999756\n",
      "  74.76000214 74.93000031 75.         74.83000183 75.23000336 75.20999908\n",
      "  75.34999847 75.11000061 73.98999786 74.08999634 73.66999817 73.55000305\n",
      "  73.54000092 73.94000244 74.70999908 74.66999817 74.08999634 75.22000122\n",
      "  75.66999817 75.97000122 76.33000183 76.36000061 76.48999786 77.58999634\n",
      "  77.41999817 77.47000122 77.66999817 77.56999969 78.31999969 78.98999786\n",
      "  78.90000153 78.58000183 79.19999695 84.37000275 83.69999695 84.36000061\n",
      "  83.68000031 83.34999847 84.08000183 84.19999695 84.76999664 84.13999939\n",
      "  84.11000061 83.79000092 83.66000366 83.5        83.47000122 83.09999847\n",
      "  83.12000275 82.40000153 82.73999786 83.83000183 83.01000214 83.30999756\n",
      "  84.06999969 84.70999908 83.51000214 83.59999847 84.41999817 81.33999634\n",
      "  81.55000305 82.54000092 83.62999725 84.29000092 85.30999756 85.73999786\n",
      "  85.43000031 85.26000214 87.12000275 86.34999847 86.19999695 86.05000305]]\n",
      "319\n"
     ]
    }
   ],
   "source": [
    " # First calculate the mid prices from the highest and lowest\n",
    "\n",
    "train_data = model_dataset[:750]\n",
    "test_data = model_dataset[750:]\n",
    "\n",
    "print(train_data.transpose())\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "687\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 63 (approx. 1 quarter) timesteps and 1 output (next day value)\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(63, len(training_set_scaled)):\n",
    "    X_train.append(training_set_scaled[i-63:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(X_train.shape[1])\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "#https://keras.io/layer/recurrent\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense #output layer\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout #for each Epox deactivate those that result in 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, fit and plot data using regressor calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "687/687 [==============================] - 5s 7ms/step - loss: 0.1045\n",
      "Epoch 2/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0296\n",
      "Epoch 3/30\n",
      "687/687 [==============================] - 2s 4ms/step - loss: 0.0186\n",
      "Epoch 4/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0112\n",
      "Epoch 5/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0069\n",
      "Epoch 6/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0048\n",
      "Epoch 7/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0045\n",
      "Epoch 8/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0045\n",
      "Epoch 9/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0039\n",
      "Epoch 10/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0043\n",
      "Epoch 11/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0040\n",
      "Epoch 12/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0037\n",
      "Epoch 13/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0040\n",
      "Epoch 14/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0033\n",
      "Epoch 15/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0035\n",
      "Epoch 16/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0036\n",
      "Epoch 17/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0036\n",
      "Epoch 18/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0037\n",
      "Epoch 19/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0034\n",
      "Epoch 20/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0038\n",
      "Epoch 21/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0038\n",
      "Epoch 22/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0037\n",
      "Epoch 23/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0035\n",
      "Epoch 24/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0038\n",
      "Epoch 25/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0041\n",
      "Epoch 26/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0036\n",
      "Epoch 27/30\n",
      "687/687 [==============================] - 2s 2ms/step - loss: 0.0034\n",
      "Epoch 28/30\n",
      "687/687 [==============================] - 2s 3ms/step - loss: 0.0038\n",
      "Epoch 29/30\n",
      "687/687 [==============================] - 2s 4ms/step - loss: 0.0036\n",
      "Epoch 30/30\n",
      "687/687 [==============================] - 3s 5ms/step - loss: 0.0031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1c3ecec88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 30, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting Real stock price\n",
    "real_stock_price = test_data\n",
    "\n",
    "# Getting the predicted stock price\n",
    "dataset_total = dataset['Close']\n",
    "inputs = dataset_total[len(dataset_total) - len(test_data) - 63:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(63, 380):\n",
    "    X_test.append(inputs[i-63:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2019-04-03 00:00:00'),\n",
       " Timestamp('2019-04-04 00:00:00'),\n",
       " Timestamp('2019-04-05 00:00:00'),\n",
       " Timestamp('2019-04-06 00:00:00'),\n",
       " Timestamp('2019-04-07 00:00:00'),\n",
       " Timestamp('2019-04-08 00:00:00'),\n",
       " Timestamp('2019-04-09 00:00:00'),\n",
       " Timestamp('2019-04-10 00:00:00'),\n",
       " Timestamp('2019-04-11 00:00:00'),\n",
       " Timestamp('2019-04-12 00:00:00'),\n",
       " Timestamp('2019-04-13 00:00:00'),\n",
       " Timestamp('2019-04-14 00:00:00'),\n",
       " Timestamp('2019-04-15 00:00:00'),\n",
       " Timestamp('2019-04-16 00:00:00'),\n",
       " Timestamp('2019-04-17 00:00:00'),\n",
       " Timestamp('2019-04-18 00:00:00'),\n",
       " Timestamp('2019-04-19 00:00:00'),\n",
       " Timestamp('2019-04-20 00:00:00'),\n",
       " Timestamp('2019-04-21 00:00:00'),\n",
       " Timestamp('2019-04-22 00:00:00'),\n",
       " Timestamp('2019-04-23 00:00:00'),\n",
       " Timestamp('2019-04-24 00:00:00'),\n",
       " Timestamp('2019-04-25 00:00:00'),\n",
       " Timestamp('2019-04-26 00:00:00'),\n",
       " Timestamp('2019-04-27 00:00:00'),\n",
       " Timestamp('2019-04-28 00:00:00'),\n",
       " Timestamp('2019-04-29 00:00:00'),\n",
       " Timestamp('2019-04-30 00:00:00'),\n",
       " Timestamp('2019-05-01 00:00:00'),\n",
       " Timestamp('2019-05-02 00:00:00'),\n",
       " Timestamp('2019-05-03 00:00:00'),\n",
       " Timestamp('2019-05-04 00:00:00'),\n",
       " Timestamp('2019-05-05 00:00:00'),\n",
       " Timestamp('2019-05-06 00:00:00'),\n",
       " Timestamp('2019-05-07 00:00:00'),\n",
       " Timestamp('2019-05-08 00:00:00'),\n",
       " Timestamp('2019-05-09 00:00:00'),\n",
       " Timestamp('2019-05-10 00:00:00'),\n",
       " Timestamp('2019-05-11 00:00:00'),\n",
       " Timestamp('2019-05-12 00:00:00'),\n",
       " Timestamp('2019-05-13 00:00:00'),\n",
       " Timestamp('2019-05-14 00:00:00'),\n",
       " Timestamp('2019-05-15 00:00:00'),\n",
       " Timestamp('2019-05-16 00:00:00'),\n",
       " Timestamp('2019-05-17 00:00:00'),\n",
       " Timestamp('2019-05-18 00:00:00'),\n",
       " Timestamp('2019-05-19 00:00:00'),\n",
       " Timestamp('2019-05-20 00:00:00'),\n",
       " Timestamp('2019-05-21 00:00:00'),\n",
       " Timestamp('2019-05-22 00:00:00'),\n",
       " Timestamp('2019-05-23 00:00:00'),\n",
       " Timestamp('2019-05-24 00:00:00'),\n",
       " Timestamp('2019-05-25 00:00:00'),\n",
       " Timestamp('2019-05-26 00:00:00'),\n",
       " Timestamp('2019-05-27 00:00:00'),\n",
       " Timestamp('2019-05-28 00:00:00'),\n",
       " Timestamp('2019-05-29 00:00:00'),\n",
       " Timestamp('2019-05-30 00:00:00'),\n",
       " Timestamp('2019-05-31 00:00:00'),\n",
       " Timestamp('2019-06-01 00:00:00'),\n",
       " Timestamp('2019-06-02 00:00:00'),\n",
       " Timestamp('2019-06-03 00:00:00'),\n",
       " Timestamp('2019-06-04 00:00:00'),\n",
       " Timestamp('2019-06-05 00:00:00'),\n",
       " Timestamp('2019-06-06 00:00:00'),\n",
       " Timestamp('2019-06-07 00:00:00'),\n",
       " Timestamp('2019-06-08 00:00:00'),\n",
       " Timestamp('2019-06-09 00:00:00'),\n",
       " Timestamp('2019-06-10 00:00:00'),\n",
       " Timestamp('2019-06-11 00:00:00'),\n",
       " Timestamp('2019-06-12 00:00:00'),\n",
       " Timestamp('2019-06-13 00:00:00'),\n",
       " Timestamp('2019-06-14 00:00:00'),\n",
       " Timestamp('2019-06-15 00:00:00'),\n",
       " Timestamp('2019-06-16 00:00:00'),\n",
       " Timestamp('2019-06-17 00:00:00'),\n",
       " Timestamp('2019-06-18 00:00:00'),\n",
       " Timestamp('2019-06-19 00:00:00'),\n",
       " Timestamp('2019-06-20 00:00:00'),\n",
       " Timestamp('2019-06-21 00:00:00'),\n",
       " Timestamp('2019-06-22 00:00:00'),\n",
       " Timestamp('2019-06-23 00:00:00'),\n",
       " Timestamp('2019-06-24 00:00:00'),\n",
       " Timestamp('2019-06-25 00:00:00'),\n",
       " Timestamp('2019-06-26 00:00:00'),\n",
       " Timestamp('2019-06-27 00:00:00'),\n",
       " Timestamp('2019-06-28 00:00:00'),\n",
       " Timestamp('2019-06-29 00:00:00'),\n",
       " Timestamp('2019-06-30 00:00:00'),\n",
       " Timestamp('2019-07-01 00:00:00'),\n",
       " Timestamp('2019-07-02 00:00:00'),\n",
       " Timestamp('2019-07-03 00:00:00'),\n",
       " Timestamp('2019-07-04 00:00:00'),\n",
       " Timestamp('2019-07-05 00:00:00'),\n",
       " Timestamp('2019-07-06 00:00:00'),\n",
       " Timestamp('2019-07-07 00:00:00'),\n",
       " Timestamp('2019-07-08 00:00:00'),\n",
       " Timestamp('2019-07-09 00:00:00'),\n",
       " Timestamp('2019-07-10 00:00:00'),\n",
       " Timestamp('2019-07-11 00:00:00'),\n",
       " Timestamp('2019-07-12 00:00:00'),\n",
       " Timestamp('2019-07-13 00:00:00'),\n",
       " Timestamp('2019-07-14 00:00:00'),\n",
       " Timestamp('2019-07-15 00:00:00'),\n",
       " Timestamp('2019-07-16 00:00:00'),\n",
       " Timestamp('2019-07-17 00:00:00'),\n",
       " Timestamp('2019-07-18 00:00:00'),\n",
       " Timestamp('2019-07-19 00:00:00'),\n",
       " Timestamp('2019-07-20 00:00:00'),\n",
       " Timestamp('2019-07-21 00:00:00'),\n",
       " Timestamp('2019-07-22 00:00:00'),\n",
       " Timestamp('2019-07-23 00:00:00'),\n",
       " Timestamp('2019-07-24 00:00:00'),\n",
       " Timestamp('2019-07-25 00:00:00'),\n",
       " Timestamp('2019-07-26 00:00:00'),\n",
       " Timestamp('2019-07-27 00:00:00'),\n",
       " Timestamp('2019-07-28 00:00:00'),\n",
       " Timestamp('2019-07-29 00:00:00'),\n",
       " Timestamp('2019-07-30 00:00:00'),\n",
       " Timestamp('2019-07-31 00:00:00'),\n",
       " Timestamp('2019-08-01 00:00:00'),\n",
       " Timestamp('2019-08-02 00:00:00'),\n",
       " Timestamp('2019-08-03 00:00:00'),\n",
       " Timestamp('2019-08-04 00:00:00'),\n",
       " Timestamp('2019-08-05 00:00:00'),\n",
       " Timestamp('2019-08-06 00:00:00'),\n",
       " Timestamp('2019-08-07 00:00:00'),\n",
       " Timestamp('2019-08-08 00:00:00'),\n",
       " Timestamp('2019-08-09 00:00:00'),\n",
       " Timestamp('2019-08-10 00:00:00'),\n",
       " Timestamp('2019-08-11 00:00:00'),\n",
       " Timestamp('2019-08-12 00:00:00'),\n",
       " Timestamp('2019-08-13 00:00:00'),\n",
       " Timestamp('2019-08-14 00:00:00'),\n",
       " Timestamp('2019-08-15 00:00:00'),\n",
       " Timestamp('2019-08-16 00:00:00'),\n",
       " Timestamp('2019-08-17 00:00:00'),\n",
       " Timestamp('2019-08-18 00:00:00'),\n",
       " Timestamp('2019-08-19 00:00:00'),\n",
       " Timestamp('2019-08-20 00:00:00'),\n",
       " Timestamp('2019-08-21 00:00:00'),\n",
       " Timestamp('2019-08-22 00:00:00'),\n",
       " Timestamp('2019-08-23 00:00:00'),\n",
       " Timestamp('2019-08-24 00:00:00'),\n",
       " Timestamp('2019-08-25 00:00:00'),\n",
       " Timestamp('2019-08-26 00:00:00'),\n",
       " Timestamp('2019-08-27 00:00:00'),\n",
       " Timestamp('2019-08-28 00:00:00'),\n",
       " Timestamp('2019-08-29 00:00:00'),\n",
       " Timestamp('2019-08-30 00:00:00'),\n",
       " Timestamp('2019-08-31 00:00:00'),\n",
       " Timestamp('2019-09-01 00:00:00'),\n",
       " Timestamp('2019-09-02 00:00:00'),\n",
       " Timestamp('2019-09-03 00:00:00'),\n",
       " Timestamp('2019-09-04 00:00:00'),\n",
       " Timestamp('2019-09-05 00:00:00'),\n",
       " Timestamp('2019-09-06 00:00:00'),\n",
       " Timestamp('2019-09-07 00:00:00'),\n",
       " Timestamp('2019-09-08 00:00:00'),\n",
       " Timestamp('2019-09-09 00:00:00'),\n",
       " Timestamp('2019-09-10 00:00:00'),\n",
       " Timestamp('2019-09-11 00:00:00'),\n",
       " Timestamp('2019-09-12 00:00:00'),\n",
       " Timestamp('2019-09-13 00:00:00'),\n",
       " Timestamp('2019-09-14 00:00:00'),\n",
       " Timestamp('2019-09-15 00:00:00'),\n",
       " Timestamp('2019-09-16 00:00:00'),\n",
       " Timestamp('2019-09-17 00:00:00'),\n",
       " Timestamp('2019-09-18 00:00:00'),\n",
       " Timestamp('2019-09-19 00:00:00'),\n",
       " Timestamp('2019-09-20 00:00:00'),\n",
       " Timestamp('2019-09-21 00:00:00'),\n",
       " Timestamp('2019-09-22 00:00:00'),\n",
       " Timestamp('2019-09-23 00:00:00'),\n",
       " Timestamp('2019-09-24 00:00:00'),\n",
       " Timestamp('2019-09-25 00:00:00'),\n",
       " Timestamp('2019-09-26 00:00:00'),\n",
       " Timestamp('2019-09-27 00:00:00'),\n",
       " Timestamp('2019-09-28 00:00:00'),\n",
       " Timestamp('2019-09-29 00:00:00'),\n",
       " Timestamp('2019-09-30 00:00:00'),\n",
       " Timestamp('2019-10-01 00:00:00'),\n",
       " Timestamp('2019-10-02 00:00:00'),\n",
       " Timestamp('2019-10-03 00:00:00'),\n",
       " Timestamp('2019-10-04 00:00:00'),\n",
       " Timestamp('2019-10-05 00:00:00'),\n",
       " Timestamp('2019-10-06 00:00:00'),\n",
       " Timestamp('2019-10-07 00:00:00'),\n",
       " Timestamp('2019-10-08 00:00:00'),\n",
       " Timestamp('2019-10-09 00:00:00'),\n",
       " Timestamp('2019-10-10 00:00:00'),\n",
       " Timestamp('2019-10-11 00:00:00'),\n",
       " Timestamp('2019-10-12 00:00:00'),\n",
       " Timestamp('2019-10-13 00:00:00'),\n",
       " Timestamp('2019-10-14 00:00:00'),\n",
       " Timestamp('2019-10-15 00:00:00'),\n",
       " Timestamp('2019-10-16 00:00:00'),\n",
       " Timestamp('2019-10-17 00:00:00'),\n",
       " Timestamp('2019-10-18 00:00:00'),\n",
       " Timestamp('2019-10-19 00:00:00'),\n",
       " Timestamp('2019-10-20 00:00:00'),\n",
       " Timestamp('2019-10-21 00:00:00'),\n",
       " Timestamp('2019-10-22 00:00:00'),\n",
       " Timestamp('2019-10-23 00:00:00'),\n",
       " Timestamp('2019-10-24 00:00:00'),\n",
       " Timestamp('2019-10-25 00:00:00'),\n",
       " Timestamp('2019-10-26 00:00:00'),\n",
       " Timestamp('2019-10-27 00:00:00'),\n",
       " Timestamp('2019-10-28 00:00:00'),\n",
       " Timestamp('2019-10-29 00:00:00'),\n",
       " Timestamp('2019-10-30 00:00:00'),\n",
       " Timestamp('2019-10-31 00:00:00'),\n",
       " Timestamp('2019-11-01 00:00:00'),\n",
       " Timestamp('2019-11-02 00:00:00'),\n",
       " Timestamp('2019-11-03 00:00:00'),\n",
       " Timestamp('2019-11-04 00:00:00'),\n",
       " Timestamp('2019-11-05 00:00:00'),\n",
       " Timestamp('2019-11-06 00:00:00'),\n",
       " Timestamp('2019-11-07 00:00:00'),\n",
       " Timestamp('2019-11-08 00:00:00'),\n",
       " Timestamp('2019-11-09 00:00:00'),\n",
       " Timestamp('2019-11-10 00:00:00'),\n",
       " Timestamp('2019-11-11 00:00:00'),\n",
       " Timestamp('2019-11-12 00:00:00'),\n",
       " Timestamp('2019-11-13 00:00:00'),\n",
       " Timestamp('2019-11-14 00:00:00'),\n",
       " Timestamp('2019-11-15 00:00:00'),\n",
       " Timestamp('2019-11-16 00:00:00'),\n",
       " Timestamp('2019-11-17 00:00:00'),\n",
       " Timestamp('2019-11-18 00:00:00'),\n",
       " Timestamp('2019-11-19 00:00:00'),\n",
       " Timestamp('2019-11-20 00:00:00'),\n",
       " Timestamp('2019-11-21 00:00:00'),\n",
       " Timestamp('2019-11-22 00:00:00'),\n",
       " Timestamp('2019-11-23 00:00:00'),\n",
       " Timestamp('2019-11-24 00:00:00'),\n",
       " Timestamp('2019-11-25 00:00:00'),\n",
       " Timestamp('2019-11-26 00:00:00'),\n",
       " Timestamp('2019-11-27 00:00:00'),\n",
       " Timestamp('2019-11-28 00:00:00'),\n",
       " Timestamp('2019-11-29 00:00:00'),\n",
       " Timestamp('2019-11-30 00:00:00'),\n",
       " Timestamp('2019-12-01 00:00:00'),\n",
       " Timestamp('2019-12-02 00:00:00'),\n",
       " Timestamp('2019-12-03 00:00:00'),\n",
       " Timestamp('2019-12-04 00:00:00'),\n",
       " Timestamp('2019-12-05 00:00:00'),\n",
       " Timestamp('2019-12-06 00:00:00'),\n",
       " Timestamp('2019-12-07 00:00:00'),\n",
       " Timestamp('2019-12-08 00:00:00'),\n",
       " Timestamp('2019-12-09 00:00:00'),\n",
       " Timestamp('2019-12-10 00:00:00'),\n",
       " Timestamp('2019-12-11 00:00:00'),\n",
       " Timestamp('2019-12-12 00:00:00'),\n",
       " Timestamp('2019-12-13 00:00:00'),\n",
       " Timestamp('2019-12-14 00:00:00'),\n",
       " Timestamp('2019-12-15 00:00:00'),\n",
       " Timestamp('2019-12-16 00:00:00'),\n",
       " Timestamp('2019-12-17 00:00:00'),\n",
       " Timestamp('2019-12-18 00:00:00'),\n",
       " Timestamp('2019-12-19 00:00:00'),\n",
       " Timestamp('2019-12-20 00:00:00'),\n",
       " Timestamp('2019-12-21 00:00:00'),\n",
       " Timestamp('2019-12-22 00:00:00'),\n",
       " Timestamp('2019-12-23 00:00:00'),\n",
       " Timestamp('2019-12-24 00:00:00'),\n",
       " Timestamp('2019-12-25 00:00:00'),\n",
       " Timestamp('2019-12-26 00:00:00'),\n",
       " Timestamp('2019-12-27 00:00:00'),\n",
       " Timestamp('2019-12-28 00:00:00'),\n",
       " Timestamp('2019-12-29 00:00:00'),\n",
       " Timestamp('2019-12-30 00:00:00'),\n",
       " Timestamp('2019-12-31 00:00:00'),\n",
       " Timestamp('2020-01-01 00:00:00'),\n",
       " Timestamp('2020-01-02 00:00:00'),\n",
       " Timestamp('2020-01-03 00:00:00'),\n",
       " Timestamp('2020-01-04 00:00:00'),\n",
       " Timestamp('2020-01-05 00:00:00'),\n",
       " Timestamp('2020-01-06 00:00:00'),\n",
       " Timestamp('2020-01-07 00:00:00'),\n",
       " Timestamp('2020-01-08 00:00:00'),\n",
       " Timestamp('2020-01-09 00:00:00'),\n",
       " Timestamp('2020-01-10 00:00:00'),\n",
       " Timestamp('2020-01-11 00:00:00'),\n",
       " Timestamp('2020-01-12 00:00:00'),\n",
       " Timestamp('2020-01-13 00:00:00'),\n",
       " Timestamp('2020-01-14 00:00:00'),\n",
       " Timestamp('2020-01-15 00:00:00'),\n",
       " Timestamp('2020-01-16 00:00:00'),\n",
       " Timestamp('2020-01-17 00:00:00'),\n",
       " Timestamp('2020-01-18 00:00:00'),\n",
       " Timestamp('2020-01-19 00:00:00'),\n",
       " Timestamp('2020-01-20 00:00:00'),\n",
       " Timestamp('2020-01-21 00:00:00'),\n",
       " Timestamp('2020-01-22 00:00:00'),\n",
       " Timestamp('2020-01-23 00:00:00'),\n",
       " Timestamp('2020-01-24 00:00:00'),\n",
       " Timestamp('2020-01-25 00:00:00'),\n",
       " Timestamp('2020-01-26 00:00:00'),\n",
       " Timestamp('2020-01-27 00:00:00'),\n",
       " Timestamp('2020-01-28 00:00:00'),\n",
       " Timestamp('2020-01-29 00:00:00'),\n",
       " Timestamp('2020-01-30 00:00:00'),\n",
       " Timestamp('2020-01-31 00:00:00'),\n",
       " Timestamp('2020-02-01 00:00:00'),\n",
       " Timestamp('2020-02-02 00:00:00'),\n",
       " Timestamp('2020-02-03 00:00:00'),\n",
       " Timestamp('2020-02-04 00:00:00'),\n",
       " Timestamp('2020-02-05 00:00:00'),\n",
       " Timestamp('2020-02-06 00:00:00'),\n",
       " Timestamp('2020-02-07 00:00:00'),\n",
       " Timestamp('2020-02-08 00:00:00'),\n",
       " Timestamp('2020-02-09 00:00:00'),\n",
       " Timestamp('2020-02-10 00:00:00'),\n",
       " Timestamp('2020-02-11 00:00:00'),\n",
       " Timestamp('2020-02-12 00:00:00'),\n",
       " Timestamp('2020-02-13 00:00:00'),\n",
       " Timestamp('2020-02-14 00:00:00'),\n",
       " Timestamp('2020-02-15 00:00:00'),\n",
       " Timestamp('2020-02-16 00:00:00'),\n",
       " Timestamp('2020-02-17 00:00:00'),\n",
       " Timestamp('2020-02-18 00:00:00'),\n",
       " Timestamp('2020-02-19 00:00:00'),\n",
       " Timestamp('2020-02-20 00:00:00'),\n",
       " Timestamp('2020-02-21 00:00:00'),\n",
       " Timestamp('2020-02-22 00:00:00'),\n",
       " Timestamp('2020-02-23 00:00:00'),\n",
       " Timestamp('2020-02-24 00:00:00'),\n",
       " Timestamp('2020-02-25 00:00:00'),\n",
       " Timestamp('2020-02-26 00:00:00'),\n",
       " Timestamp('2020-02-27 00:00:00'),\n",
       " Timestamp('2020-02-28 00:00:00'),\n",
       " Timestamp('2020-02-29 00:00:00'),\n",
       " Timestamp('2020-03-01 00:00:00'),\n",
       " Timestamp('2020-03-02 00:00:00'),\n",
       " Timestamp('2020-03-03 00:00:00'),\n",
       " Timestamp('2020-03-04 00:00:00'),\n",
       " Timestamp('2020-03-05 00:00:00'),\n",
       " Timestamp('2020-03-06 00:00:00'),\n",
       " Timestamp('2020-03-07 00:00:00'),\n",
       " Timestamp('2020-03-08 00:00:00'),\n",
       " Timestamp('2020-03-09 00:00:00'),\n",
       " Timestamp('2020-03-10 00:00:00'),\n",
       " Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2020-03-12 00:00:00'),\n",
       " Timestamp('2020-03-13 00:00:00'),\n",
       " Timestamp('2020-03-14 00:00:00'),\n",
       " Timestamp('2020-03-15 00:00:00'),\n",
       " Timestamp('2020-03-16 00:00:00'),\n",
       " Timestamp('2020-03-17 00:00:00'),\n",
       " Timestamp('2020-03-18 00:00:00'),\n",
       " Timestamp('2020-03-19 00:00:00'),\n",
       " Timestamp('2020-03-20 00:00:00'),\n",
       " Timestamp('2020-03-21 00:00:00'),\n",
       " Timestamp('2020-03-22 00:00:00'),\n",
       " Timestamp('2020-03-23 00:00:00'),\n",
       " Timestamp('2020-03-24 00:00:00'),\n",
       " Timestamp('2020-03-25 00:00:00'),\n",
       " Timestamp('2020-03-26 00:00:00'),\n",
       " Timestamp('2020-03-27 00:00:00'),\n",
       " Timestamp('2020-03-28 00:00:00'),\n",
       " Timestamp('2020-03-29 00:00:00'),\n",
       " Timestamp('2020-03-30 00:00:00'),\n",
       " Timestamp('2020-03-31 00:00:00')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_dates = [dataset.index[-1] + DateOffset(days=x) for x in range(1,365)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 63, 50)            10400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 63, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 63, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 63, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 63, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 71,051\n",
      "Trainable params: 71,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(regressor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ef724e7090ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_dates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfuture_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forecast'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_dates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have 3 dimensions, but got array with shape ()"
     ]
    }
   ],
   "source": [
    "x = np.array(future_dates.index)\n",
    "\n",
    "future_dates['forecast'] = regressor.predict(np.array(future_dates.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
