{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Solution adapted from Kris C Naik\n",
    "#GitHub https://github.com/krishnaik06/Stock-Price-Prediction-using-Keras-and-Recurrent-Neural-Networ/blob/master/rnn.py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from pandas import concat\n",
    "\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "#from app.model.preprocessor import Preprocessor as img_prep #image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader as dr\n",
    "from datetime import datetime\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from scipy import misc\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start=datetime(2015,1,1)\n",
    "end=datetime(2019,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>47.419998</td>\n",
       "      <td>46.540001</td>\n",
       "      <td>46.660000</td>\n",
       "      <td>46.759998</td>\n",
       "      <td>27913900.0</td>\n",
       "      <td>42.418739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>46.730000</td>\n",
       "      <td>46.250000</td>\n",
       "      <td>46.369999</td>\n",
       "      <td>46.330002</td>\n",
       "      <td>39673900.0</td>\n",
       "      <td>42.028660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>46.750000</td>\n",
       "      <td>45.540001</td>\n",
       "      <td>46.380001</td>\n",
       "      <td>45.650002</td>\n",
       "      <td>36447900.0</td>\n",
       "      <td>41.411785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>46.459999</td>\n",
       "      <td>45.490002</td>\n",
       "      <td>45.980000</td>\n",
       "      <td>46.230000</td>\n",
       "      <td>29114100.0</td>\n",
       "      <td>41.937946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>47.750000</td>\n",
       "      <td>46.720001</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>47.590000</td>\n",
       "      <td>29645200.0</td>\n",
       "      <td>43.171677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low       Open      Close      Volume  Adj Close\n",
       "Date                                                                         \n",
       "2015-01-02  47.419998  46.540001  46.660000  46.759998  27913900.0  42.418739\n",
       "2015-01-05  46.730000  46.250000  46.369999  46.330002  39673900.0  42.028660\n",
       "2015-01-06  46.750000  45.540001  46.380001  45.650002  36447900.0  41.411785\n",
       "2015-01-07  46.459999  45.490002  45.980000  46.230000  29114100.0  41.937946\n",
       "2015-01-08  47.750000  46.720001  46.750000  47.590000  29645200.0  43.171677"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tesla = web.DataReader('TSLA', 'google', start,end)\n",
    "microsoft = dr.data.get_data_yahoo('MSFT', start=start, end=end)\n",
    "\n",
    "dataset = microsoft\n",
    "close_data = microsoft['Close']\n",
    "dates = pd.DataFrame(close_data.index)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1):\n",
    "    df = pd.DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values\n",
    "\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "806/806 [==============================] - 1s 684us/sample - loss: 82056293721142272.0000 - acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "806/806 [==============================] - 0s 132us/sample - loss: 170429283389938.0312 - acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "806/806 [==============================] - 0s 134us/sample - loss: 118471424484893.2188 - acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "806/806 [==============================] - 0s 174us/sample - loss: 82353685174063.6406 - acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "806/806 [==============================] - 0s 132us/sample - loss: 57246968669440.6328 - acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "806/806 [==============================] - 0s 131us/sample - loss: 39794389945199.1641 - acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "806/806 [==============================] - 0s 130us/sample - loss: 27662482904391.7852 - acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "806/806 [==============================] - 0s 142us/sample - loss: 19229171473761.1914 - acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "806/806 [==============================] - 0s 182us/sample - loss: 13366877114378.1621 - acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "806/806 [==============================] - 0s 187us/sample - loss: 9291785947486.6504 - acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "806/806 [==============================] - 0s 127us/sample - loss: 6459047350891.9902 - acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "806/806 [==============================] - 0s 174us/sample - loss: 4489911330129.9453 - acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "806/806 [==============================] - 0s 202us/sample - loss: 3121095726204.5063 - acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "806/806 [==============================] - 0s 157us/sample - loss: 2169583929491.3748 - acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "806/806 [==============================] - 0s 140us/sample - loss: 1508154460022.7891 - acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "806/806 [==============================] - 0s 166us/sample - loss: 1048371418724.3673 - acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "806/806 [==============================] - 0s 138us/sample - loss: 728760318211.1761 - acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "806/806 [==============================] - 0s 158us/sample - loss: 506586789252.7642 - acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "806/806 [==============================] - 0s 170us/sample - loss: 352146491458.0645 - acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "806/806 [==============================] - 0s 161us/sample - loss: 244789242836.8040 - acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "806/806 [==============================] - 0s 196us/sample - loss: 170161678300.4268 - acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "806/806 [==============================] - 0s 124us/sample - loss: 118285466184.4169 - acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "806/806 [==============================] - 0s 135us/sample - loss: 82224473822.3325 - acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "806/806 [==============================] - 0s 147us/sample - loss: 57157122246.1936 - acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "806/806 [==============================] - 0s 171us/sample - loss: 39731887848.4963 - acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "806/806 [==============================] - 0s 137us/sample - loss: 27619083464.7345 - acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "806/806 [==============================] - 0s 151us/sample - loss: 19198983368.7345 - acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "806/806 [==============================] - 0s 115us/sample - loss: 13345858654.0149 - acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "806/806 [==============================] - 0s 137us/sample - loss: 9277171079.3052 - acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "806/806 [==============================] - 0s 121us/sample - loss: 6448836948.4864 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227241ee940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = 200  #Test Sample\n",
    "n_input = 63  #time lag \n",
    "n_filter = 8  #filters to use in prediction\n",
    "n_epochs = 30 #iterations\n",
    "n_batch = 100 #simulatenous evaluation\n",
    "n_kernel = 3  #\n",
    "train, test = train_test_split(dataset['Close'], n_test)\n",
    "\n",
    "\n",
    "data = series_to_supervised(train, n_in=n_input)\n",
    "train_x, train_y = data[:, :-1], data[:, -1]\n",
    "                \n",
    "train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=8,kernel_size=n_kernel,kernel_initializer='VarianceScaling', activation='relu', \n",
    "                        input_shape=(n_input,1)))\n",
    "model.add(Conv1D(filters=16,kernel_size=5,strides=1, kernel_initializer='VarianceScaling', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# model.add(Dense(units=10, kernel_initializer='VarianceScaling',activation='softmax'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=n_batch, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 46.75999832  46.33000183  45.65000153  46.22999954  47.59000015\n  47.18999863  46.59999847  46.36000061  45.95999908  45.47999954\n  46.24000168  46.38999939  45.91999817  47.13000107  47.18000031\n  47.00999832  42.65999985  41.18999863  42.00999832  40.40000153\n  41.27999878  41.59999847  41.84000015  42.45000076  42.40999985\n  42.36000061  42.59999847  42.38000107  43.09000015  43.86999893\n  43.58000183  43.52999878  43.5         43.86000061  44.15000153\n  44.09000015  43.99000168  44.06000137  43.84999847  43.88000107\n  43.27999878  43.06000137  43.11000061  42.36000061  42.84999847\n  42.02999878  41.97999954  41.02000046  41.38000107  41.56000137\n  41.70000076  42.5         42.29000092  42.88000107  42.86000061\n  42.90000153  41.45999908  41.20999908  40.97000122  40.95999908\n  40.65999985  40.72000122  40.29000092  41.54999924  41.52999878\n  41.41999817  41.47999954  41.72000122  41.75999832  41.65000153\n  42.25999832  42.15999985  41.61999893  42.90999985  42.63999939\n  42.99000168  43.34000015  47.86999893  48.02999878  49.15999985\n  49.06000137  48.63999939  48.65999985  48.24000168  47.59999847\n  46.27999878  46.70000076  47.75        47.36999893  47.34999847\n  47.63000107  48.72000122  48.29999924  48.00999832  47.58000183\n  47.58000183  47.41999817  46.90000153  46.59000015  47.61000061\n  47.45000076  46.86000061  47.22999954  46.91999817  46.84999847\n  46.36000061  46.13999939  45.72999954  45.65000153  46.61000061\n  46.43999863  45.97000122  45.47999954  45.83000183  45.97000122\n  46.72000122  46.09999847  46.22999954  45.90999985  45.63999939\n  45.65000153  45.25999832  44.36999893  44.15000153  44.45000076\n  44.40000153  44.38999939  44.29999924  44.24000168  44.52000046\n  44.61000061  45.54000092  45.61999893  45.75999832  46.65999985\n  46.61999893  46.91999817  47.27999878  45.54000092  46.11000061\n  45.93999863  45.34999847  45.34000015  46.29000092  46.88000107\n  46.70000076  46.81000137  47.54000092  47.58000183  46.61999893\n  46.74000168  47.33000183  46.40999985  46.74000168  46.72999954\n  47.          47.31999969  47.27000046  46.61000061  45.65999985\n  43.06999969  41.68000031  40.47000122  42.70999908  43.90000153\n  43.93000031  43.52000046  41.81999969  43.36000061  43.5\n  42.61000061  43.88999939  43.06999969  43.29000092  43.47999954\n  43.04000092  43.97999954  44.29999924  44.25        43.47999954\n  44.11000061  43.90000153  43.86999893  43.90999985  43.93999863\n  43.29000092  43.43999863  44.25999832  44.61000061  45.56999969\n  46.63000107  46.75        46.79999924  47.45000076  47.11000061\n  47.          46.88999939  46.68000031  47.00999832  47.50999832\n  47.61999893  47.77000046  47.20000076  48.02999878  52.86999893\n  54.25        53.68999863  53.97999954  53.36000061  52.63999939\n  53.24000168  54.15000153  54.40000153  54.38000107  54.91999817\n  54.15999985  53.50999832  53.65000153  53.31999969  52.84000015\n  53.77000046  52.97000122  53.84999847  53.93999863  54.18999863\n  54.18999863  54.25        53.68999863  53.93000031  54.34999847\n  55.22000122  55.20999908  54.20000076  55.90999985  55.81000137\n  55.79000092  54.97999954  55.27000046  54.06000137  55.13999939\n  55.20000076  56.13000107  55.70000076  54.13000107  54.83000183\n  55.34999847  55.81999969  55.66999817  55.95000076  56.54999924\n  56.31000137  55.47999954  54.79999924  55.04999924  54.04999924\n  52.16999817  52.33000183  52.29999924  52.77999878  51.63999939\n  53.11000061  50.99000168  50.56000137  50.79000092  50.47999954\n  52.29000092  51.79000092  52.16999817  51.22000122  52.06000137\n  55.09000015  54.70999908  53.          52.15999985  52.\n  50.15999985  49.40999985  49.27999878  49.70999908  49.68999863\n  50.5         51.09000015  52.41999817  52.18999863  51.81999969\n  52.65000153  51.18000031  51.36000061  52.09999847  51.29999924\n  50.88000107  52.58000183  52.95000076  52.34999847  52.02999878\n  51.02999878  51.65000153  52.84000015  52.04999924  53.06999969\n  53.16999817  53.59000015  54.34999847  54.65999985  53.49000168\n  53.86000061  54.06999969  53.97000122  54.20999908  53.54000092\n  54.70999908  55.04999924  55.22999954  55.56999969  55.43000031\n  54.56000137  55.11999893  54.45999908  54.41999817  54.31000137\n  54.65000153  55.34999847  55.36000061  55.65000153  56.45999908\n  56.38999939  55.59000015  55.77999878  51.77999878  52.11000061\n  51.43999863  50.93999863  49.90000153  49.86999893  50.61000061\n  49.77999878  49.86999893  49.93999863  50.38999939  50.06999969\n  51.02000046  51.04999924  51.50999832  51.08000183  51.83000183\n  50.50999832  50.81000137  50.31999969  50.61999893  50.02999878\n  51.59000015  52.11999893  51.88999939  52.31999969  53.\n  52.84999847  52.47999954  51.79000092  52.13000107  52.09999847\n  52.04000092  51.61999893  51.47999954  50.13999939  49.83000183\n  49.68999863  50.38999939  50.13000107  50.06999969  51.18999863\n  50.99000168  51.90999985  49.83000183  48.43000031  49.43999863\n  50.54000092  51.16999817  51.15999985  51.16999817  51.38000107\n  51.38000107  52.29999924  52.59000015  53.20999908  53.50999832\n  53.74000168  53.70000076  53.95999908  53.09000015  55.90999985\n  55.79999924  56.56999969  56.72999954  56.75999832  56.18999863\n  56.20999908  56.68000031  56.58000183  56.58000183  56.97000122\n  57.38999939  57.95999908  58.06000137  58.20000076  58.02000046\n  58.29999924  57.93999863  58.11999893  57.43999863  57.56000137\n  57.59999847  57.61999893  57.66999817  57.88999939  57.95000076\n  58.16999817  58.02999878  58.09999847  57.88999939  57.45999908\n  57.59000015  57.66999817  57.61000061  57.65999985  57.43000031\n  56.20999908  57.04999924  56.52999878  56.25999832  57.18999863\n  57.25        56.93000031  56.81000137  57.75999832  57.81999969\n  57.43000031  56.90000153  57.95000076  58.02999878  57.40000153\n  57.59999847  57.41999817  57.24000168  57.63999939  57.74000168\n  57.79999924  58.04000092  57.18999863  57.11000061  56.91999817\n  57.41999817  57.22000122  57.65999985  57.52999878  57.25\n  59.65999985  61.          60.99000168  60.63000107  60.09999847\n  59.86999893  59.91999817  59.79999924  59.43000031  59.20999908\n  58.70999908  60.41999817  60.47000122  60.16999817  58.70000076\n  59.02000046  58.11999893  58.86999893  59.65000153  60.63999939\n  60.34999847  60.86000061  61.11999893  60.40000153  60.52999878\n  60.61000061  61.09000015  60.25999832  59.20000076  59.25\n  60.22000122  59.95000076  61.36999893  61.00999832  61.97000122\n  62.16999817  62.97999954  62.68000031  62.58000183  62.29999924\n  63.61999893  63.54000092  63.54000092  63.54999924  63.24000168\n  63.27999878  62.99000168  62.90000153  62.13999939  62.58000183\n  62.29999924  62.29999924  62.84000015  62.63999939  62.61999893\n  63.18999863  62.61000061  62.70000076  62.52999878  62.5\n  62.29999924  62.74000168  62.95999908  63.52000046  63.68000031\n  64.26999664  65.77999878  65.12999725  64.65000153  63.58000183\n  63.16999817  63.68000031  63.63999939  63.43000031  63.34000015\n  64.05999756  64.          64.72000122  64.56999969  64.52999878\n  64.51999664  64.62000275  64.48999786  64.36000061  64.62000275\n  64.62000275  64.23000336  63.97999954  64.94000244  64.01000214\n  64.25        64.26999664  64.40000153  64.98999786  64.73000336\n  64.93000031  64.70999908  64.41000366  64.75        64.63999939\n  64.87000275  64.93000031  64.20999908  65.02999878  64.87000275\n  64.98000336  65.09999847  65.29000092  65.47000122  65.70999908\n  65.86000061  65.55000305  65.73000336  65.55999756  65.73000336\n  65.68000031  65.52999878  65.48000336  65.23000336  64.94999695\n  65.48000336  65.38999939  65.04000092  65.5         66.40000153\n  67.52999878  67.91999817  67.83000183  68.26999664  68.45999908\n  69.41000366  69.30000305  69.08000183  68.80999756  69.\n  68.94000244  69.04000092  69.30999756  68.45999908  68.37999725\n  68.43000031  69.41000366  67.48000336  67.70999908  67.69000244\n  68.44999695  68.68000031  68.76999664  69.62000275  69.95999908\n  70.41000366  69.83999634  70.09999847  71.76000214  72.27999878\n  72.51999664  72.38999939  71.94999695  70.31999969  69.77999878\n  70.65000153  70.26999664  69.90000153  70.          70.87000275\n  69.91000366  70.26999664  70.26000214  71.20999908  70.52999878\n  69.20999908  69.80000305  68.48999786  68.93000031  68.16999817\n  69.08000183  68.56999969  69.45999908  69.98000336  69.98999786\n  71.15000153  71.76999664  72.77999878  73.34999847  73.30000305\n  73.86000061  74.22000122  73.79000092  73.59999847  74.19000244\n  74.05000305  73.16000366  73.04000092  72.69999695  72.58000183\n  72.26000214  72.15000153  72.68000031  72.40000153  72.79000092\n  72.47000122  71.41000366  72.5         73.58999634  73.22000122\n  73.65000153  72.40000153  72.48999786  72.15000153  73.16000366\n  72.72000122  72.69000244  72.81999969  72.83000183  73.05000305\n  74.01000214  74.76999664  73.94000244  73.61000061  73.40000153\n  74.33999634  73.98000336  74.76000214  74.68000031  75.20999908\n  74.76999664  75.30999756  75.16000366  75.44000244  74.94000244\n  74.20999908  74.41000366  73.26000214  73.26000214  73.84999847\n  73.87000275  74.48999786  74.61000061  74.26000214  74.69000244\n  75.97000122  76.          76.29000092  76.29000092  76.41999817\n  77.12000275  77.48999786  77.65000153  77.58999634  77.61000061\n  77.91000366  78.80999756  78.83000183  78.86000061  78.62999725\n  78.76000214  83.80999756  83.88999939  83.18000031  83.18000031\n  84.05000305  84.13999939  84.47000122  84.26999664  84.55999756\n  84.08999634  83.87000275  83.93000031  84.05000305  82.98000336\n  83.19999695  82.40000153  82.52999878  83.72000122  83.11000061\n  83.26000214  83.87000275  84.87999725  83.33999634  84.16999817\n  84.26000214  81.08000183  81.58999634  82.77999878  82.48999786\n  84.16000366  85.23000336  85.58000183  85.34999847  84.69000244\n  86.84999847  86.37999725  85.83000183  85.51999664  85.5\n  85.51000214  85.40000153  85.70999908  85.72000122  85.54000092\n  85.94999695  86.34999847  87.11000061  88.19000244  88.27999878\n  88.22000122  87.81999969  88.08000183  89.59999847  88.34999847\n  90.13999939  90.09999847  90.          91.61000061  91.90000153\n  91.81999969  92.33000183  94.05999756  93.91999817  92.73999786\n  95.01000214  94.26000214  91.77999878  88.          91.33000183\n  89.61000061  85.01000214  88.18000031  89.12999725  89.83000183\n  90.80999756  92.66000366  92.          92.72000122  91.48999786\n  91.73000336  94.05999756  95.41999817  94.19999695  93.76999664\n  92.84999847  93.05000305  93.63999939  93.31999969  93.86000061\n  94.43000031  96.54000092  96.76999664  94.41000366  93.84999847\n  94.18000031  94.59999847  92.88999939  93.12999725  92.48000336\n  89.79000092  87.18000031  93.77999878  89.47000122  89.38999939\n  91.26999664  88.51999664  89.70999908  92.33000183  92.37999725\n  90.23000336  90.76999664  92.87999725  91.86000061  93.58000183\n  93.08000183  94.16999817  96.06999969  96.44000244  96.11000061\n  95.          95.34999847  93.12000275  92.30999756  94.26000214\n  95.81999969  93.51999664  95.          93.51000214  94.06999969\n  95.16000366  96.22000122  95.80999756  96.94000244  97.91000366\n  97.69999695  98.02999878  97.31999969  97.15000153  96.18000031\n  96.36000061  97.59999847  97.5         98.66000366  98.30999756\n  98.36000061  98.01000214  98.94999695  98.83999634 100.79000092\n 101.66999817 102.19000244 102.48999786 100.87999725 101.62999725\n 101.05000305 101.30999756 100.84999847 101.41999817].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5a9bb00b6b08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mreal_stock_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtraining_set_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m--> 334\u001b[1;33m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 46.75999832  46.33000183  45.65000153  46.22999954  47.59000015\n  47.18999863  46.59999847  46.36000061  45.95999908  45.47999954\n  46.24000168  46.38999939  45.91999817  47.13000107  47.18000031\n  47.00999832  42.65999985  41.18999863  42.00999832  40.40000153\n  41.27999878  41.59999847  41.84000015  42.45000076  42.40999985\n  42.36000061  42.59999847  42.38000107  43.09000015  43.86999893\n  43.58000183  43.52999878  43.5         43.86000061  44.15000153\n  44.09000015  43.99000168  44.06000137  43.84999847  43.88000107\n  43.27999878  43.06000137  43.11000061  42.36000061  42.84999847\n  42.02999878  41.97999954  41.02000046  41.38000107  41.56000137\n  41.70000076  42.5         42.29000092  42.88000107  42.86000061\n  42.90000153  41.45999908  41.20999908  40.97000122  40.95999908\n  40.65999985  40.72000122  40.29000092  41.54999924  41.52999878\n  41.41999817  41.47999954  41.72000122  41.75999832  41.65000153\n  42.25999832  42.15999985  41.61999893  42.90999985  42.63999939\n  42.99000168  43.34000015  47.86999893  48.02999878  49.15999985\n  49.06000137  48.63999939  48.65999985  48.24000168  47.59999847\n  46.27999878  46.70000076  47.75        47.36999893  47.34999847\n  47.63000107  48.72000122  48.29999924  48.00999832  47.58000183\n  47.58000183  47.41999817  46.90000153  46.59000015  47.61000061\n  47.45000076  46.86000061  47.22999954  46.91999817  46.84999847\n  46.36000061  46.13999939  45.72999954  45.65000153  46.61000061\n  46.43999863  45.97000122  45.47999954  45.83000183  45.97000122\n  46.72000122  46.09999847  46.22999954  45.90999985  45.63999939\n  45.65000153  45.25999832  44.36999893  44.15000153  44.45000076\n  44.40000153  44.38999939  44.29999924  44.24000168  44.52000046\n  44.61000061  45.54000092  45.61999893  45.75999832  46.65999985\n  46.61999893  46.91999817  47.27999878  45.54000092  46.11000061\n  45.93999863  45.34999847  45.34000015  46.29000092  46.88000107\n  46.70000076  46.81000137  47.54000092  47.58000183  46.61999893\n  46.74000168  47.33000183  46.40999985  46.74000168  46.72999954\n  47.          47.31999969  47.27000046  46.61000061  45.65999985\n  43.06999969  41.68000031  40.47000122  42.70999908  43.90000153\n  43.93000031  43.52000046  41.81999969  43.36000061  43.5\n  42.61000061  43.88999939  43.06999969  43.29000092  43.47999954\n  43.04000092  43.97999954  44.29999924  44.25        43.47999954\n  44.11000061  43.90000153  43.86999893  43.90999985  43.93999863\n  43.29000092  43.43999863  44.25999832  44.61000061  45.56999969\n  46.63000107  46.75        46.79999924  47.45000076  47.11000061\n  47.          46.88999939  46.68000031  47.00999832  47.50999832\n  47.61999893  47.77000046  47.20000076  48.02999878  52.86999893\n  54.25        53.68999863  53.97999954  53.36000061  52.63999939\n  53.24000168  54.15000153  54.40000153  54.38000107  54.91999817\n  54.15999985  53.50999832  53.65000153  53.31999969  52.84000015\n  53.77000046  52.97000122  53.84999847  53.93999863  54.18999863\n  54.18999863  54.25        53.68999863  53.93000031  54.34999847\n  55.22000122  55.20999908  54.20000076  55.90999985  55.81000137\n  55.79000092  54.97999954  55.27000046  54.06000137  55.13999939\n  55.20000076  56.13000107  55.70000076  54.13000107  54.83000183\n  55.34999847  55.81999969  55.66999817  55.95000076  56.54999924\n  56.31000137  55.47999954  54.79999924  55.04999924  54.04999924\n  52.16999817  52.33000183  52.29999924  52.77999878  51.63999939\n  53.11000061  50.99000168  50.56000137  50.79000092  50.47999954\n  52.29000092  51.79000092  52.16999817  51.22000122  52.06000137\n  55.09000015  54.70999908  53.          52.15999985  52.\n  50.15999985  49.40999985  49.27999878  49.70999908  49.68999863\n  50.5         51.09000015  52.41999817  52.18999863  51.81999969\n  52.65000153  51.18000031  51.36000061  52.09999847  51.29999924\n  50.88000107  52.58000183  52.95000076  52.34999847  52.02999878\n  51.02999878  51.65000153  52.84000015  52.04999924  53.06999969\n  53.16999817  53.59000015  54.34999847  54.65999985  53.49000168\n  53.86000061  54.06999969  53.97000122  54.20999908  53.54000092\n  54.70999908  55.04999924  55.22999954  55.56999969  55.43000031\n  54.56000137  55.11999893  54.45999908  54.41999817  54.31000137\n  54.65000153  55.34999847  55.36000061  55.65000153  56.45999908\n  56.38999939  55.59000015  55.77999878  51.77999878  52.11000061\n  51.43999863  50.93999863  49.90000153  49.86999893  50.61000061\n  49.77999878  49.86999893  49.93999863  50.38999939  50.06999969\n  51.02000046  51.04999924  51.50999832  51.08000183  51.83000183\n  50.50999832  50.81000137  50.31999969  50.61999893  50.02999878\n  51.59000015  52.11999893  51.88999939  52.31999969  53.\n  52.84999847  52.47999954  51.79000092  52.13000107  52.09999847\n  52.04000092  51.61999893  51.47999954  50.13999939  49.83000183\n  49.68999863  50.38999939  50.13000107  50.06999969  51.18999863\n  50.99000168  51.90999985  49.83000183  48.43000031  49.43999863\n  50.54000092  51.16999817  51.15999985  51.16999817  51.38000107\n  51.38000107  52.29999924  52.59000015  53.20999908  53.50999832\n  53.74000168  53.70000076  53.95999908  53.09000015  55.90999985\n  55.79999924  56.56999969  56.72999954  56.75999832  56.18999863\n  56.20999908  56.68000031  56.58000183  56.58000183  56.97000122\n  57.38999939  57.95999908  58.06000137  58.20000076  58.02000046\n  58.29999924  57.93999863  58.11999893  57.43999863  57.56000137\n  57.59999847  57.61999893  57.66999817  57.88999939  57.95000076\n  58.16999817  58.02999878  58.09999847  57.88999939  57.45999908\n  57.59000015  57.66999817  57.61000061  57.65999985  57.43000031\n  56.20999908  57.04999924  56.52999878  56.25999832  57.18999863\n  57.25        56.93000031  56.81000137  57.75999832  57.81999969\n  57.43000031  56.90000153  57.95000076  58.02999878  57.40000153\n  57.59999847  57.41999817  57.24000168  57.63999939  57.74000168\n  57.79999924  58.04000092  57.18999863  57.11000061  56.91999817\n  57.41999817  57.22000122  57.65999985  57.52999878  57.25\n  59.65999985  61.          60.99000168  60.63000107  60.09999847\n  59.86999893  59.91999817  59.79999924  59.43000031  59.20999908\n  58.70999908  60.41999817  60.47000122  60.16999817  58.70000076\n  59.02000046  58.11999893  58.86999893  59.65000153  60.63999939\n  60.34999847  60.86000061  61.11999893  60.40000153  60.52999878\n  60.61000061  61.09000015  60.25999832  59.20000076  59.25\n  60.22000122  59.95000076  61.36999893  61.00999832  61.97000122\n  62.16999817  62.97999954  62.68000031  62.58000183  62.29999924\n  63.61999893  63.54000092  63.54000092  63.54999924  63.24000168\n  63.27999878  62.99000168  62.90000153  62.13999939  62.58000183\n  62.29999924  62.29999924  62.84000015  62.63999939  62.61999893\n  63.18999863  62.61000061  62.70000076  62.52999878  62.5\n  62.29999924  62.74000168  62.95999908  63.52000046  63.68000031\n  64.26999664  65.77999878  65.12999725  64.65000153  63.58000183\n  63.16999817  63.68000031  63.63999939  63.43000031  63.34000015\n  64.05999756  64.          64.72000122  64.56999969  64.52999878\n  64.51999664  64.62000275  64.48999786  64.36000061  64.62000275\n  64.62000275  64.23000336  63.97999954  64.94000244  64.01000214\n  64.25        64.26999664  64.40000153  64.98999786  64.73000336\n  64.93000031  64.70999908  64.41000366  64.75        64.63999939\n  64.87000275  64.93000031  64.20999908  65.02999878  64.87000275\n  64.98000336  65.09999847  65.29000092  65.47000122  65.70999908\n  65.86000061  65.55000305  65.73000336  65.55999756  65.73000336\n  65.68000031  65.52999878  65.48000336  65.23000336  64.94999695\n  65.48000336  65.38999939  65.04000092  65.5         66.40000153\n  67.52999878  67.91999817  67.83000183  68.26999664  68.45999908\n  69.41000366  69.30000305  69.08000183  68.80999756  69.\n  68.94000244  69.04000092  69.30999756  68.45999908  68.37999725\n  68.43000031  69.41000366  67.48000336  67.70999908  67.69000244\n  68.44999695  68.68000031  68.76999664  69.62000275  69.95999908\n  70.41000366  69.83999634  70.09999847  71.76000214  72.27999878\n  72.51999664  72.38999939  71.94999695  70.31999969  69.77999878\n  70.65000153  70.26999664  69.90000153  70.          70.87000275\n  69.91000366  70.26999664  70.26000214  71.20999908  70.52999878\n  69.20999908  69.80000305  68.48999786  68.93000031  68.16999817\n  69.08000183  68.56999969  69.45999908  69.98000336  69.98999786\n  71.15000153  71.76999664  72.77999878  73.34999847  73.30000305\n  73.86000061  74.22000122  73.79000092  73.59999847  74.19000244\n  74.05000305  73.16000366  73.04000092  72.69999695  72.58000183\n  72.26000214  72.15000153  72.68000031  72.40000153  72.79000092\n  72.47000122  71.41000366  72.5         73.58999634  73.22000122\n  73.65000153  72.40000153  72.48999786  72.15000153  73.16000366\n  72.72000122  72.69000244  72.81999969  72.83000183  73.05000305\n  74.01000214  74.76999664  73.94000244  73.61000061  73.40000153\n  74.33999634  73.98000336  74.76000214  74.68000031  75.20999908\n  74.76999664  75.30999756  75.16000366  75.44000244  74.94000244\n  74.20999908  74.41000366  73.26000214  73.26000214  73.84999847\n  73.87000275  74.48999786  74.61000061  74.26000214  74.69000244\n  75.97000122  76.          76.29000092  76.29000092  76.41999817\n  77.12000275  77.48999786  77.65000153  77.58999634  77.61000061\n  77.91000366  78.80999756  78.83000183  78.86000061  78.62999725\n  78.76000214  83.80999756  83.88999939  83.18000031  83.18000031\n  84.05000305  84.13999939  84.47000122  84.26999664  84.55999756\n  84.08999634  83.87000275  83.93000031  84.05000305  82.98000336\n  83.19999695  82.40000153  82.52999878  83.72000122  83.11000061\n  83.26000214  83.87000275  84.87999725  83.33999634  84.16999817\n  84.26000214  81.08000183  81.58999634  82.77999878  82.48999786\n  84.16000366  85.23000336  85.58000183  85.34999847  84.69000244\n  86.84999847  86.37999725  85.83000183  85.51999664  85.5\n  85.51000214  85.40000153  85.70999908  85.72000122  85.54000092\n  85.94999695  86.34999847  87.11000061  88.19000244  88.27999878\n  88.22000122  87.81999969  88.08000183  89.59999847  88.34999847\n  90.13999939  90.09999847  90.          91.61000061  91.90000153\n  91.81999969  92.33000183  94.05999756  93.91999817  92.73999786\n  95.01000214  94.26000214  91.77999878  88.          91.33000183\n  89.61000061  85.01000214  88.18000031  89.12999725  89.83000183\n  90.80999756  92.66000366  92.          92.72000122  91.48999786\n  91.73000336  94.05999756  95.41999817  94.19999695  93.76999664\n  92.84999847  93.05000305  93.63999939  93.31999969  93.86000061\n  94.43000031  96.54000092  96.76999664  94.41000366  93.84999847\n  94.18000031  94.59999847  92.88999939  93.12999725  92.48000336\n  89.79000092  87.18000031  93.77999878  89.47000122  89.38999939\n  91.26999664  88.51999664  89.70999908  92.33000183  92.37999725\n  90.23000336  90.76999664  92.87999725  91.86000061  93.58000183\n  93.08000183  94.16999817  96.06999969  96.44000244  96.11000061\n  95.          95.34999847  93.12000275  92.30999756  94.26000214\n  95.81999969  93.51999664  95.          93.51000214  94.06999969\n  95.16000366  96.22000122  95.80999756  96.94000244  97.91000366\n  97.69999695  98.02999878  97.31999969  97.15000153  96.18000031\n  96.36000061  97.59999847  97.5         98.66000366  98.30999756\n  98.36000061  98.01000214  98.94999695  98.83999634 100.79000092\n 101.66999817 102.19000244 102.48999786 100.87999725 101.62999725\n 101.05000305 101.30999756 100.84999847 101.41999817].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Setting Real stock price\n",
    "real_stock_price = test\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "print(train.head())\n",
    "history = [x for x in train]\n",
    "\n",
    "training_set = dataset.iloc[:, 1:2].values\n",
    "print(training_set.info)\n",
    "\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "# predicted_stock_price = regressor.predict(X_test)\n",
    "x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "# forecast\n",
    "predicted_stock_price = model.predict(x_input, verbose=0)\n",
    "\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "future_dates = [dataset.index[-1] + DateOffset(days=x) for x in range(1,365)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(regressor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(future_dates.index)\n",
    "\n",
    "future_dates['forecast'] = regressor.predict(np.array(future_dates.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
